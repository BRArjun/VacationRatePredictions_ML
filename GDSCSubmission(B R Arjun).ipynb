{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnAcyebmhhSW"
      },
      "outputs": [],
      "source": [
        "import numpy as np  #For Linear Algebra\n",
        "import csv  #For creation of the final CSV file\n",
        "import pandas as pd  #Data Processing and I/O Operations done on the CSV File\n",
        "\n",
        "# Gradient XGBoost with Random Forest for making predictions\n",
        "from xgboost import XGBRFRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH3PVPDGFZ_q"
      },
      "source": [
        "âš›: The reason I went for XGBoost with Random Forest Regression as my preferred model is:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **Ensemble Power** : The combination of XGBoost with Random Forest can be a useful tool to combine the strengths of both the models, and hence improve regression accuracy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "2.   **Robust to Overfitting** : Both the models come with built in methods to combat overfitting with XGBoost having boosting and Random Forest having bagging.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3.   **Non Linearity Handling** : Both the models are suitable to work on a wide range of problems as they are able to capture non-linearities effectively and understand the data better. They are also useful for handling various types of data and can be useful even when there are missing values.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4.   **Hyperparameter Tuning** : Both the models allow the user to optimize the model further by tweaking Hyperparameters for the model and tailor a model specific to the task on hand.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5.   **Parallelization** : Both the models support parallelization that helps train the model at a faster rate on Multi-core CPUs and good GPUs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Overall this combination of XGBoost implemented with Random Forest can be very effective in solving a wide range of regression problems with a lot of opportunities to optimize the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPB_bRjYFZ_t"
      },
      "outputs": [],
      "source": [
        "# Define dataset\n",
        "file_path=\"/content/train.csv\"\n",
        "weather_data=pd.read_csv(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4yKRrlTFZ_t"
      },
      "source": [
        "# Defining the **features and target variables** for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V68hVQ2JFZ_u"
      },
      "outputs": [],
      "source": [
        "features=[\"YEAR\",\"T2M_RANGE\",\"T2M_MAX\",\"T2M_MIN\",\"RH2M\",\"PS\",\"WS10M\"]\n",
        "X=weather_data[features]\n",
        "\n",
        "\n",
        "targets=[\"T2M\",\"QV2M\",\"VACATION_RATE\"]\n",
        "y=weather_data[targets]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDzPHSUJFZ_u"
      },
      "source": [
        "# Defining the **test dataset** and the **features** for the model to analyze and predict from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baZmHWBuFZ_u"
      },
      "outputs": [],
      "source": [
        "test_file_path='/content/test.csv'\n",
        "weather_data_test=pd.read_csv(test_file_path)\n",
        "\n",
        "test_features=[\"YEAR\",\"T2M_RANGE\",\"T2M_MAX\",\"T2M_MIN\",\"RH2M\",\"PS\",\"WS10M\"]\n",
        "X_test=weather_data_test[test_features]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSN-3cR6FZ_u"
      },
      "source": [
        "# Defining the Model\n",
        "**HyperParameters**\n",
        "\n",
        "1. n_estimators :  The number of boosting rounds or trees in the model. A higher number can improve performance, but can also cause overfitting of the data.\n",
        "\n",
        "2. subsample : The fraction of samples used for fitting the trees. It helps control overfitting.\n",
        "\n",
        "3. colsample_bynode :  It controls the fraction of features that are considered when constructing each tree node during the boosting process.\n",
        "\n",
        "4. learning_rate :  It controls the step size at each iteration when updating the model's parameters to minimize the loss function.\n",
        "\n",
        "5. max_depth :  It specifies the maximum depth or level that a decision tree can grow to during the construction process.\n",
        "\n",
        "6. random_state : It serves the purpose of introducing randomness into certain operations that would otherwise be deterministic. Its primary use is to control the randomness or reproducibility of a machine learning model's behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuVFCItkFZ_v"
      },
      "outputs": [],
      "source": [
        "model = XGBRFRegressor(n_estimators=1000, subsample=0.9, colsample_bynode=0.1 , learning_rate=0.8, max_depth=7,random_state=0)\n",
        "\n",
        "\n",
        "# Fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "\n",
        "# Start predictions\n",
        "yhat = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Uq6zedFZ_w"
      },
      "source": [
        "# Using List Comprehension to take out VACATION_RATE values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klyfsDMKjX2A"
      },
      "outputs": [],
      "source": [
        "vacation_rate = [row[-1] for row in yhat]\n",
        "\n",
        "#CREATING THE LIST OF IDS\n",
        "ids = [1440 + i for i in range(len(vacation_rate))]\n",
        "\n",
        "#MAPPING EACH ID TO EVERY VALUE OF VACATION_RATE\n",
        "data = list(zip(ids, vacation_rate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdtALG-MFZ_w"
      },
      "source": [
        "# CREATING A NEW CSV FILE CONTAINING ID AND VACATION_RATE ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShYHmBUeFZ_x"
      },
      "outputs": [],
      "source": [
        "csv_file_path = \"/content/submission.csv\"\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "\n",
        "    # Write header\n",
        "    csv_writer.writerow([\"ID\", \"VACATION_RATE\"])\n",
        "\n",
        "\n",
        "    # Write data rows\n",
        "    csv_writer.writerows(data)\n",
        "\n",
        "\n",
        "print(\"CSV file has been created.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
